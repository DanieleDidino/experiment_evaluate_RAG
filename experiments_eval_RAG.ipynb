{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Evaluating RAG systems \n",
    "\n",
    "In this notebook, I test the `RagDatasetGenerator` and the `RagEvaluatorPack` classes to evaluate a RAG systems.\n",
    "\n",
    "I use as external document for the RAG the paper \n",
    "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n",
    "(only the first 2 pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import environ\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index import ServiceContext, StorageContext\n",
    "from llama_index import OpenAIEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.llama_dataset.generator import RagDatasetGenerator # \"RagDatasetGenerator\" requires the library \"spacy\"\n",
    "from llama_index.llama_pack import download_llama_pack\n",
    "import nest_asyncio\n",
    "\n",
    "env = environ.Env()\n",
    "environ.Env.read_env()\n",
    "API_KEY = env('OPENAI_API_KEY')\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "RagEvaluatorPack = download_llama_pack(\"RagEvaluatorPack\", \"./pack\")\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "docs_path = \"documents_pdf\"\n",
    "\n",
    "# Set LLM and model for embedding \n",
    "llm = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "def load_docs(doc_path):\n",
    "    docs = SimpleDirectoryReader(input_dir=doc_path).load_data()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(docs, llm, embed_model):\n",
    "    \"\"\"\n",
    "    Build an index (vector database using the VectorStoreIndex class of LlamaIndex).\n",
    "\n",
    "    Args:\n",
    "        docs (Document): An object of the Document class from LlamaIndex.\n",
    "        llm: OpenAI Chat large language models API.\n",
    "            Example: llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "        embed_model: OpenAI embedding models.\n",
    "            Example: llm_emb = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "    Returns:\n",
    "        index (VectorStoreIndex): An object of the VectorStoreIndex class from \n",
    "            LlamaIndex to use to build a query engine.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Define The ServiceContext: a bundle of commonly used resources used during\n",
    "    # the indexing and querying stage in a LlamaIndex pipeline/application.\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Storage context: The storage context container is a utility container for storing \n",
    "    # nodes, indices, and vectors. It contains the following:\n",
    "    # - docstore: BaseDocumentStore\n",
    "    # - index_store: BaseIndexStore\n",
    "    # - vector_store: VectorStore\n",
    "    # - graph_store: GraphStore\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # VectorStoreIndex: a data structure that allows for the retrieval of relevant context\n",
    "    # for a user query. This is particularly useful for retrieval-augmented generation (RAG) use-cases.\n",
    "    # VectorStoreIndex stores data in Node objects, which represent chunks of the original documents,\n",
    "    # and exposes a Retriever interface that supports additional configuration and automation.\n",
    "    print(\"Creating Vector Database ...\")\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=docs,\n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context,\n",
    "        show_progress=True\n",
    "    )\n",
    "    print(\"Done\")\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_all = load_docs(docs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in this notebooks we are not interested in the evaluating the RAG system but to create an example for a RAG eval pipeline,\n",
    "I only select 2 pages from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = docs_all[0:2]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/Desktop/Projects/experiment_evaluate_RAG/.venv_rag_eval/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vector Database ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 2/2 [00:00<00:00, 559.43it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index = create_vector_db(docs, llm, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context_rag_dataset = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 2/2 [00:00<00:00, 527.15it/s]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.85s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# \"RagDatasetGenerator\" requires the library \"spacy\"\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents=docs,\n",
    "    service_context=service_context_rag_dataset,\n",
    "    # set the number of questions per nodes\n",
    "    # (Each document is chunked of size 512 words)\n",
    "    num_questions_per_chunk=3, # 10,  \n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "rag_dataset = dataset_generator.generate_dataset_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since there are 28 nodes, there should be a total of 84 questions\n",
    "len(dataset_generator.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rag_dataset.dict()[\"examples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of BERT in natural language processing tasks?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset[0].query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based andﬁne-tuning . The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning allpre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset[0].reference_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The purpose of BERT in natural language processing tasks is to pretrain deep bidirectional representations from unlabeled text by conditioning on both left and right context in all layers. This allows the pre-trained BERT model to be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset[0].reference_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of BERT in natural languag...</td>\n",
       "      <td>[BERT: Pre-training of Deep Bidirectional Tran...</td>\n",
       "      <td>The purpose of BERT in natural language proces...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does BERT improve the limitations of previ...</td>\n",
       "      <td>[BERT: Pre-training of Deep Bidirectional Tran...</td>\n",
       "      <td>BERT improves the limitations of previous lang...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the masked language model pre-training...</td>\n",
       "      <td>[BERT: Pre-training of Deep Bidirectional Tran...</td>\n",
       "      <td>The masked language model (MLM) pre-training o...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the main objective of BERT's pre-train...</td>\n",
       "      <td>[word based only on its context. Unlike left-t...</td>\n",
       "      <td>The main objective of BERT's pre-training appr...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does BERT's pre-trained representations re...</td>\n",
       "      <td>[word based only on its context. Unlike left-t...</td>\n",
       "      <td>BERT's pre-trained representations reduce the ...</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  What is the purpose of BERT in natural languag...   \n",
       "1  How does BERT improve the limitations of previ...   \n",
       "2  What is the masked language model pre-training...   \n",
       "3  What is the main objective of BERT's pre-train...   \n",
       "4  How does BERT's pre-trained representations re...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [BERT: Pre-training of Deep Bidirectional Tran...   \n",
       "1  [BERT: Pre-training of Deep Bidirectional Tran...   \n",
       "2  [BERT: Pre-training of Deep Bidirectional Tran...   \n",
       "3  [word based only on its context. Unlike left-t...   \n",
       "4  [word based only on its context. Unlike left-t...   \n",
       "\n",
       "                                    reference_answer reference_answer_by  \\\n",
       "0  The purpose of BERT in natural language proces...  ai (gpt-3.5-turbo)   \n",
       "1  BERT improves the limitations of previous lang...  ai (gpt-3.5-turbo)   \n",
       "2  The masked language model (MLM) pre-training o...  ai (gpt-3.5-turbo)   \n",
       "3  The main objective of BERT's pre-training appr...  ai (gpt-3.5-turbo)   \n",
       "4  BERT's pre-trained representations reduce the ...  ai (gpt-3.5-turbo)   \n",
       "\n",
       "             query_by  \n",
       "0  ai (gpt-3.5-turbo)  \n",
       "1  ai (gpt-3.5-turbo)  \n",
       "2  ai (gpt-3.5-turbo)  \n",
       "3  ai (gpt-3.5-turbo)  \n",
       "4  ai (gpt-3.5-turbo)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of questions-response evaluated by `RagEvaluatorPack` is\n",
    "\n",
    "N_nodes * num_questions_per_chunk\n",
    "\n",
    "with a `batch_size` of 10 (default)\n",
    "\n",
    "The default judge model in `RagEvaluatorPack` is  `OpenAI(temperature=0, model=\"gpt-4-1106-preview\")`,\n",
    "which consumes much more credits compared to `OpenAI(temperature=0, model=\"gpt-3.5-turbo\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_evaluator = RagEvaluatorPack(\n",
    "    query_engine=query_engine,  # built with the same source documents as the rag_dataset\n",
    "    rag_dataset=rag_dataset,\n",
    "    judge_llm=OpenAI(temperature=0, model=\"gpt-3.5-turbo\") # default: OpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `rag_evaluator_pack.run()` saves 2 files in the same directory in which the pack was invoked:\n",
    "- benchmark.csv (CSV format of the benchmark scores)\n",
    "- _evaluations.json (raw evaluation results for all examples & predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:08<00:00,  7.61s/it]\n",
      "2it [00:16,  8.13s/it]\n",
      "2it [00:14,  7.37s/it]\n",
      "2it [00:15,  7.60s/it]\n",
      "2it [00:15,  7.98s/it]\n",
      "1it [00:10, 10.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# benchmark_df = await rag_evaluator.run() ## TypeError: object DataFrame can't be used in \"await\" expression\n",
    "benchmark_df = rag_evaluator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>base_rag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_correctness_score</th>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_relevancy_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_similarity_score</th>\n",
       "      <td>0.969672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                            base_rag\n",
       "metrics                                \n",
       "mean_correctness_score         4.333333\n",
       "mean_relevancy_score           1.000000\n",
       "mean_faithfulness_score        1.000000\n",
       "mean_context_similarity_score  0.969672"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_chatbot_afa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
